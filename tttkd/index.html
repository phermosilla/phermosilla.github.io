<html> <head> <meta charset="utf-8"/> <meta name="description" content="TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models."/> <meta name="keywords" content="Test-time training, Point clouds"/> <meta name="viewport" content="width=device-width, initial-scale=1"/> <title>TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models</title> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"/> <link rel="stylesheet" href="/assets/projects/tttkd/static/css/bulma.min.css"/> <link rel="stylesheet" href="/assets/projects/tttkd/static/css/bulma-carousel.min.css"/> <link rel="stylesheet" href="/assets/projects/tttkd/static/css/bulma-slider.min.css"/> <link rel="stylesheet" href="/assets/projects/tttkd/static/css/fontawesome.all.min.css"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"/> <link rel="stylesheet" href="/assets/projects/tttkd/static/css/index.css"/> <link rel="icon" href="/assets/projects/tttkd/static/images/favicon.svg"/> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script defer="" src="/assets/projects/tttkd/static/js/fontawesome.all.min.js"></script> <script src="/assets/projects/tttkd/static/js/bulma-carousel.min.js"></script> <script src="/assets/projects/tttkd/static/js/bulma-slider.min.js"></script> </head> <body> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-3 publication-title">TTT-KD: Test-Time Training for 3D Semantic Segmentation<br>through Knowledge Distillation from Foundation Models</h1> <h2 class="subtitle is-4 publication-subtitle">3DV 2025</h2> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://cvl.tuwien.ac.at/staff/lisa-magdalena-weijler/" target="_blank" rel="noopener noreferrer">Lisa Weijler</a><sup>1</sup>,</span> <span class="author-block"> <a href="https://utkarshsinha.com" target="_blank" rel="noopener noreferrer">M. Jehanzeb Mirza</a><sup>2</sup>,</span> <span class="author-block"> <a href="https://leonsick.github.io/" target="_blank" rel="noopener noreferrer">Leon Sick</a><sup>3</sup>, </span> <span class="author-block"> Can Ekkazan<sup>4</sup>, </span> <span class="author-block"> <a href="https://phermosilla.github.io/">Pedro Hermosilla</a><sup>1</sup>, </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"><sup>1</sup>TU Wien,</span> <span class="author-block"><sup>2</sup>MIT CSAIL,</span> <span class="author-block"><sup>3</sup>Ulm University,</span> <span class="author-block"><sup>4</sup>Yildiz Technical University</span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://arxiv.org/abs/2403.11691" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener noreferrer"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span> <span class="link-block"> <a href="https://phermosilla.github.io/tttkd/" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="hero teaser"> <div class="container is-max-desktop"> <div class="hero-body"> <img src="/assets/projects/tttkd/static/images/teaser.gif" alt="teaser"> <h2 class="subtitle has-text-centered"> <b>TTT-KD</b> is the first test-time training method for 3D semantic segmentation which adapts to distribution shifts at test time. </h2> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2 class="title is-3">Abstract</h2> <div class="content has-text-justified"> <p> Test-Time Training (TTT) proposes to adapt a pre-trained network to changing data distributions on-the-fly. In this work, we propose the first TTT method for 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD) from foundation models (e.g. DINOv2) as a self-supervised objective for adaptation to distribution shifts at test-time. Given access to paired image-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for the main task of semantic segmentation using the pointclouds and the task of 2D → 3D KD by using an off-the-shelf 2D pre-trained foundation model. At test-time, our TTT-KD updates the 3D segmentation backbone for each test sample, by using the self-supervised task of knowledge distillation, before performing the final prediction. Extensive evaluations on multiple indoor and outdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improves performance for both in-distribution (ID) and out-of-distribution (OOD) test datasets. We achieve a gain of up to 13 % mIoU (7 % on average) when the train and test distributions are similar and up to 45 % (20 % on average) when adapting to OOD test samples. </p> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h2 class="title is-3">Method Overview</h2> <div class="content has-text-justified"> <img src="/assets/projects/tttkd/static/images/overview.jpg" alt="overview"> </div> <div class="content has-text-justified"> <p> Given paired image-pointcloud data of a 3D scene, TTT-KD , during joint-training, optimizes the parameters of a point or voxel-based 3D backbone, ψ<sub>3D</sub> , followed by two projectors, ρ<sub>Y</sub> and ρ<sub>2D</sub>. While ρ<sub>Y</sub> predicts the semantic label of each point, ρ<sub>2D</sub> is used for knowledge distillation from a frozen 2D foundation model, ϕ<sub>2D</sub>. During test-time training, for each test scene, we perform several optimization steps on the self-supervised task of knowledge distillation to fine-tune the parameters of the 3D backbone. Lastly, during inference, we freeze all parameters of the model to perform the final prediction. By improving on the knowledge distillation task during TTT, the model adapts to out-of-distribution 3D scenes different from the source data the model was initially trained on. On the off-line version of our algorithm, we reset the parameter updates after each inference step. On the online version, we keep the updates after each inference, allowing us to reduce the number of TTT steps required. </p> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h2 class="title is-3">Results</h2> <h3 class="title is-4">Main Results</h3> <div class="content has-text-justified"> <img src="/assets/projects/tttkd/static/images/main_results.png" alt="main_results"> </div> <div class="content has-text-justified"> <p> <b>Joint-training.</b> Our results show that for all datasets and both 3D backbones, <i>Joint-Train</i> always provides an improvement over the <i>Source-Only</i> model. </p> <p> <b>In-distribution.</b> When testing on in-distribution data, train and test on the same dataset, our algorithm, <i>TTT-KD</i>, provides significant improvements for all three datasets and all 3D backbones. </p> <p> <b>Out-of-distribution.</b> When we look at the performance of the <i>Source-Only</i> models when tested on ODD data, as expected, the performance drops significantly when compared with an <i>Oracle</i> model trained on ID data. Our <i>TTT-KD</i> algorithm, on the other hand, is able to reduce this gap, increasing significantly the performance of all models and even obtaining better performance than the <i>Oracle</i> model as in the ScanNet → Matterport3D experiment. </p> <p> <b>Comparison to baselines.</b> When compared to <i>TENT</i>, <i>DUA</i>, <i>AdaContrast</i>, and <i>TTT-MAE</i>, although these baselines can provide some adaptation, <i>TTT-KD</i> has a clear advantage, surpassing them by a large margin. </p> <p> <b>Backbone agnostic.</b> When we analyze the performance of our method on different backbones, we see a consistent improvement in all setups. This indicates that our method is independent of the 3D backbone used. </p> </div> <br> <h3 class="title is-4">Qualitative Results</h3> <div class="content has-text-justified"> <img src="/assets/projects/tttkd/static/images/qualitative_1.jpg" alt="qualitative_results"> </div> <br> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column is-full-width"> <h2 class="title is-3">Ablations</h2> <div class="content has-text-justified"> <img src="/assets/projects/tttkd/static/images/ablations.png" alt="ablations"> </div> <br> <div class="content has-text-justified"> <p> <b>Number of steps.</b> We measure the performance of our TTT-KD algorithm in relationship to the number of TTT steps and plot the results in Fig. (a). We see that the mIoU increases with the number of TTT updates, and saturates at 200 steps. However, relative improvement is reduced after 100 steps. </p> <p> <b>Number of images.</b> We measure the performance of our method w.r.t. the number of images used for KD. Fig. (b) presents the results of this experiment. We can see that even when only a single image is used (most of the points in the scene do not have a paired image), we can achieve a boost in mIoU. Moreover, we can see that the improvement saturates for 5 images when the improvement obtained by including an additional image is reduced. </p> <p> <b>Foundation model.</b> We compare the foundation model used in our main experiments, DINOv2, to a CLIP model, and to a SAM model, and provide the results in Fig. (c). Our TTT-KD is also able to provide considerable performance gains while using CLIP or SAM in our pipeline. </p> </div> <br> </div> </div> </div> </section> <section class="section" id="BibTeX"> <div class="container is-max-desktop content"> <h2 class="title">BibTeX</h2> <pre><code>@article{weijler2025tttkd,
      title={TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models},
      author={Weijler, L. and Mirza, M. J. and Sick, L. and Ekkazan, C. and Hermosilla, P.},
      journal={International Conference on 3D Vision (3DV)},
      year={2025}
    }</code></pre> </div> </section> <footer class="footer"> <div class="container"> <div class="content has-text-centered"> <a class="icon-link" href="https://arxiv.org/abs/2403.11691" target="_blank" rel="noopener noreferrer"> <i class="fas fa-file-pdf"></i> </a> <a class="external-link" href="https://phermosilla.github.io/tttkd/" disabled> <i class="fab fa-github"></i> </a> </div> <div class="columns is-centered"> <div class="column is-8"> <div class="content"> <p> This website is licensed under a <a rel="license noopener noreferrer" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p> <p> Original template from <a href="https://github.com/nerfies/nerfies.github.io" target="_blank" rel="noopener noreferrer">nerfies</a>. </p> </div> </div> </div> </div> </footer> </body> </html>